{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 4: Causality Analysis (Granger & VAR)\n",
                "**Goal:** Establish causal relationship between Fed sentiment and bond market movements.\n",
                "\n",
                "## Improved Methodology\n",
                "1. **Time Series Preprocessing**: Handle non-stationarity and seasonality\n",
                "2. **VAR Model**: Vector Autoregression for multivariate analysis\n",
                "3. **Granger Causality**: Test for predictive causality\n",
                "4. **Impulse Response**: Analyze shock propagation\n",
                "5. **Control Variables**: Include macroeconomic indicators"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ! pip install statsmodels\n",
                "# ! pip  install arch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from statsmodels.tsa.api import VAR\n",
                "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
                "from statsmodels.tsa.seasonal import seasonal_decompose\n",
                "from arch import arch_model\n",
                "import yfinance as yf\n",
                "from datetime import datetime, timedelta\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: '../data/fomc_roberta_monthly_index.csv'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load Fed Sentiment Data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fed_sentiment = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/fomc_roberta_monthly_index.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m fed_sentiment[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(fed_sentiment[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      4\u001b[39m fed_sentiment = fed_sentiment.set_index(\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m).sort_index()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Administrator\\anaconda3\\envs\\textmining\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Administrator\\anaconda3\\envs\\textmining\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Administrator\\anaconda3\\envs\\textmining\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Administrator\\anaconda3\\envs\\textmining\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Administrator\\anaconda3\\envs\\textmining\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
                        "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/fomc_roberta_monthly_index.csv'"
                    ]
                }
            ],
            "source": [
                "# Load Fed Sentiment Data\n",
                "fed_sentiment = pd.read_csv('../data/processed/fomc_roberta_monthly_index.csv')\n",
                "fed_sentiment['date'] = pd.to_datetime(fed_sentiment['date'])\n",
                "fed_sentiment = fed_sentiment.set_index('date').sort_index()\n",
                "\n",
                "# Load Market Data\n",
                "start_date = fed_sentiment.index.min() - timedelta(days=30)\n",
                "end_date = fed_sentiment.index.max() + timedelta(days=30)\n",
                "\n",
                "# Download TNX (10-year Treasury yield) and MOVE (bond market volatility)\n",
                "market_data = yf.download(['^TNX', '^MOVE'], start=start_date, end=end_date)\n",
                "market_data = market_data['Close'].dropna()\n",
                "\n",
                "# Download control variables\n",
                "control_data = yf.download(['^VIX', 'DX-Y.NYB'], start=start_date, end=end_date)\n",
                "control_data = control_data['Close'].dropna()\n",
                "\n",
                "print(f\"Fed sentiment data: {len(fed_sentiment)} observations\")\n",
                "print(f\"Market data: {len(market_data)} observations\")\n",
                "print(f\"Control data: {len(control_data)} observations\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge and resample to monthly\n",
                "combined_data = pd.concat([\n",
                "    fed_sentiment['sentiment_index'],\n",
                "    market_data.resample('M').last(),\n",
                "    control_data.resample('M').last()\n",
                "], axis=1).dropna()\n",
                "\n",
                "# Rename columns\n",
                "combined_data.columns = ['fed_sentiment', 'tnx_yield', 'move_volatility', 'vix', 'dollar_index']\n",
                "\n",
                "print(\"Combined dataset shape:\", combined_data.shape)\n",
                "print(\"\\nFirst few rows:\")\n",
                "display(combined_data.head())\n",
                "\n",
                "# Plot the time series\n",
                "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
                "combined_data.plot(subplots=True, ax=axes.flatten()[:5])\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stationarity Test Function\n",
                "def test_stationarity(series, name):\n",
                "    result = adfuller(series.dropna())\n",
                "    print(f'\\n{name} Stationarity Test:')\n",
                "    print(f'ADF Statistic: {result[0]:.4f}')\n",
                "    print(f'p-value: {result[1]:.4f}')\n",
                "    print(f'Critical Values:')\n",
                "    for key, value in result[4].items():\n",
                "        print(f'\\t{key}: {value:.4f}')\n",
                "    \n",
                "    if result[1] < 0.05:\n",
                "        print(\"\u2713 Stationary\")\n",
                "    else:\n",
                "        print(\"\u2717 Non-stationary\")\n",
                "    \n",
                "    return result[1] < 0.05\n",
                "\n",
                "# Test stationarity for all variables\n",
                "stationary_results = {}\n",
                "for col in combined_data.columns:\n",
                "    stationary_results[col] = test_stationarity(combined_data[col], col)\n",
                "\n",
                "# Difference non-stationary series\n",
                "diff_data = combined_data.copy()\n",
                "for col, is_stationary in stationary_results.items():\n",
                "    if not is_stationary:\n",
                "        diff_data[f'{col}_diff'] = combined_data[col].diff().dropna()\n",
                "        print(f\"\\nDifferenced {col}\")\n",
                "\n",
                "print(\"\\nFinal dataset shape:\", diff_data.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select variables for VAR (prefer differenced if non-stationary)\n",
                "var_variables = []\n",
                "for col in ['fed_sentiment', 'tnx_yield', 'move_volatility', 'vix', 'dollar_index']:\n",
                "    if stationary_results[col]:\n",
                "        var_variables.append(col)\n",
                "    else:\n",
                "        var_variables.append(f'{col}_diff')\n",
                "\n",
                "# Drop NA values from differencing\n",
                "var_data = diff_data[var_variables].dropna()\n",
                "\n",
                "print(\"Variables for VAR analysis:\", var_variables)\n",
                "print(\"VAR dataset shape:\", var_data.shape)\n",
                "\n",
                "# Determine optimal lag length\n",
                "model = VAR(var_data)\n",
                "lag_selection = model.select_order(maxlags=12)\n",
                "print(\"\\nLag order selection:\")\n",
                "print(lag_selection.summary())\n",
                "\n",
                "# Fit VAR model\n",
                "optimal_lags = lag_selection.aic\n",
                "var_model = model.fit(optimal_lags)\n",
                "print(f\"\\nFitted VAR({optimal_lags}) model\")\n",
                "print(var_model.summary())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Granger Causality Tests\n",
                "def granger_test(data, x, y, max_lags=8):\n",
                "    print(f\"\\n=== Granger Causality: {x} \u2192 {y} ===\")\n",
                "    try:\n",
                "        test_result = grangercausalitytests(data[[y, x]], max_lags, verbose=False)\n",
                "        \n",
                "        # Get F-test p-values for different lags\n",
                "        p_values = []\n",
                "        for lag in range(1, max_lags+1):\n",
                "            f_test = test_result[lag][0]['ssr_ftest']\n",
                "            p_values.append(f_test[1])\n",
                "        \n",
                "        min_p = min(p_values)\n",
                "        best_lag = p_values.index(min_p) + 1\n",
                "        \n",
                "        print(f\"Minimum p-value: {min_p:.4f} (at lag {best_lag})\")\n",
                "        \n",
                "        if min_p < 0.05:\n",
                "            print(f\"\u2713 {x} Granger-causes {y} (significant at 5% level)\")\n",
                "        else:\n",
                "            print(f\"\u2717 No Granger causality from {x} to {y}\")\n",
                "        \n",
                "        return min_p, best_lag\n",
                "    \n",
                "    except Exception as e:\n",
                "        print(f\"Error in Granger test: {e}\")\n",
                "        return None, None\n",
                "\n",
                "# Test causality between Fed sentiment and market variables\n",
                "causality_results = []\n",
                "for target in ['tnx_yield', 'move_volatility']:\n",
                "    for source in ['fed_sentiment']:\n",
                "        p_val, lag = granger_test(var_data, source, target)\n",
                "        if p_val is not None:\n",
                "            causality_results.append({\n",
                "                'source': source,\n",
                "                'target': target,\n",
                "                'p_value': p_val,\n",
                "                'optimal_lag': lag,\n",
                "                'significant': p_val < 0.05\n",
                "            })\n",
                "\n",
                "# Display results\n",
                "results_df = pd.DataFrame(causality_results)\n",
                "print(\"\\n=== Summary of Granger Causality Tests ===\")\n",
                "display(results_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Impulse Response Analysis\n",
                "def plot_irf(var_model, impulse, response, periods=12):\n",
                "    irf = var_model.irf(periods)\n",
                "    \n",
                "    # Find column indices\n",
                "    impulse_idx = var_model.names.index(impulse)\n",
                "    response_idx = var_model.names.index(response)\n",
                "    \n",
                "    # Plot impulse response\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    irf.plot(impulse=impulse_idx, response=response_idx, \n",
                "             orth=True,  # Orthogonalized IRF\n",
                "             plot_stderr=True)\n",
                "    plt.title(f'Impulse Response: {impulse} \u2192 {response}')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.show()\n",
                "\n",
                "# Plot IRFs for significant relationships\n",
                "for result in causality_results:\n",
                "    if result['significant']:\n",
                "        impulse = result['source']\n",
                "        response = result['target']\n",
                "        print(f\"\\nPlotting IRF: {impulse} \u2192 {response}\")\n",
                "        plot_irf(var_model, impulse, response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variance Decomposition\n",
                "def plot_variance_decomp(var_model, periods=12):\n",
                "    fevd = var_model.fevd(periods)\n",
                "    \n",
                "    fig, axes = plt.subplots(len(var_model.names), len(var_model.names), \n",
                "                            figsize=(15, 15))\n",
                "    \n",
                "    for i in range(len(var_model.names)):\n",
                "        for j in range(len(var_model.names)):\n",
                "            if len(var_model.names) == 1:\n",
                "                ax = axes\n",
                "            elif len(var_model.names) > 1:\n",
                "                ax = axes[i, j]\n",
                "            \n",
                "            fevd.plot(i, j, ax=ax)\n",
                "            ax.set_title(f'{var_model.names[j]} explained by {var_model.names[i]}')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "print(\"\\n=== Forecast Error Variance Decomposition ===\")\n",
                "plot_variance_decomp(var_model)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "textmining",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}